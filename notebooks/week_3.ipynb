{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install river -q --quiet\n",
        "print(\"‚úÖ river library installed successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdBlhBAAK319",
        "outputId": "721a26ef-9bb6-4aab-af0e-10201f96ad3a"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ river library installed successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Setup and Data Upload"
      ],
      "metadata": {
        "id": "n22BYSImBO55"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-5uW9cxBJi4",
        "outputId": "de5f23b9-00fc-423f-c20b-e488b1d8ee97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Libraries imported.\n",
            "\n",
            "‚úÖ Common setup complete. You can now run any of the model blocks below.\n"
          ]
        }
      ],
      "source": [
        "# --- 1. SETUP AND IMPORTS ---\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from google.colab import files\n",
        "import numpy as np\n",
        "import warnings\n",
        "\n",
        "# Suppress ConvergenceWarning for cleaner output\n",
        "warnings.filterwarnings('ignore', category=UserWarning)\n",
        "print(\"‚úÖ Libraries imported.\")\n",
        "\n",
        "# --- 2. FILE UPLOAD ---\n",
        "# print(\"\\n‚¨ÜÔ∏è Please upload 'KDDTrain+.txt' and 'KDDTest+.txt' if you haven't already.\")\n",
        "# uploaded = files.upload()\n",
        "train_file = 'KDDTrain+.txt'\n",
        "test_file = 'KDDTest+.txt'\n",
        "\n",
        "# --- 3. PREPARE ENCODERS AND METADATA ---\n",
        "columns = [\n",
        "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n",
        "    'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n",
        "    'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n",
        "    'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login',\n",
        "    'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n",
        "    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n",
        "    'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
        "    'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',\n",
        "    'attack', 'difficulty'\n",
        "]\n",
        "categorical_cols = ['protocol_type', 'service', 'flag']\n",
        "numerical_cols = [col for col in columns if col not in categorical_cols + ['attack', 'difficulty']]\n",
        "\n",
        "# Fit encoders on all possible categories from both files\n",
        "df_train_cat = pd.read_csv(train_file, header=None, names=columns, usecols=categorical_cols)\n",
        "df_test_cat = pd.read_csv(test_file, header=None, names=columns, usecols=categorical_cols)\n",
        "combined_cat = pd.concat([df_train_cat, df_test_cat], ignore_index=True)\n",
        "\n",
        "protocol_encoder = LabelEncoder().fit(combined_cat['protocol_type'])\n",
        "service_encoder = LabelEncoder().fit(combined_cat['service'])\n",
        "flag_encoder = LabelEncoder().fit(combined_cat['flag'])\n",
        "\n",
        "all_classes = np.array([0, 1])\n",
        "chunksize = 2048 # Use a larger, more stable chunksize\n",
        "\n",
        "print(\"\\n‚úÖ Common setup complete. You can now run any of the model blocks below.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Preprocessing and Model Initialization"
      ],
      "metadata": {
        "id": "X2sDrORcBSaV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. DATA AND MODEL INITIALIZATION ---\n",
        "\n",
        "# Define column names for the dataset, as they are not in the file\n",
        "columns = [\n",
        "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n",
        "    'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n",
        "    'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells',\n",
        "    'num_access_files', 'num_outbound_cmds', 'is_host_login', 'is_guest_login',\n",
        "    'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate',\n",
        "    'same_srv_rate', 'diff_srv_rate', 'srv_diff_host_rate', 'dst_host_count',\n",
        "    'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate', 'dst_host_serror_rate',\n",
        "    'dst_host_srv_serror_rate', 'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',\n",
        "    'attack', 'difficulty'\n",
        "]\n",
        "\n",
        "# Identify categorical and numerical columns for preprocessing\n",
        "categorical_cols = ['protocol_type', 'service', 'flag']\n",
        "numerical_cols = [col for col in columns if col not in categorical_cols + ['attack', 'difficulty']]\n",
        "\n",
        "# Initialize tools for preprocessing\n",
        "# LabelEncoders to convert categorical text to numbers\n",
        "protocol_encoder = LabelEncoder()\n",
        "service_encoder = LabelEncoder()\n",
        "flag_encoder = LabelEncoder()\n",
        "\n",
        "# We need to fit the encoders on all possible categories they might see.\n",
        "# We'll combine the unique values from both train and test sets for a robust fit.\n",
        "df_train_cat = pd.read_csv(train_file, header=None, names=columns, usecols=categorical_cols)\n",
        "df_test_cat = pd.read_csv(test_file, header=None, names=columns, usecols=categorical_cols)\n",
        "combined_cat = pd.concat([df_train_cat, df_test_cat], ignore_index=True)\n",
        "\n",
        "protocol_encoder.fit(combined_cat['protocol_type'])\n",
        "service_encoder.fit(combined_cat['service'])\n",
        "flag_encoder.fit(combined_cat['flag'])\n",
        "\n",
        "# StandardScaler to scale numerical features. It will be fitted incrementally.\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Define the models that support online learning via the `partial_fit` method\n",
        "models = {\n",
        "    # Stochastic Gradient Descent with log loss is equivalent to Logistic Regression\n",
        "    \"Online Logistic Regression (SGD)\": SGDClassifier(loss='log_loss', random_state=42, learning_rate='adaptive', eta0=0.01),\n",
        "    # MLPClassifier is a neural network, inherently suited for online learning\n",
        "    \"Multi-layer Perceptron\": MLPClassifier(hidden_layer_sizes=(500, 50), activation='relu', solver='adam', random_state=42, learning_rate_init=0.01),\n",
        "    # Gaussian Naive Bayes can be updated incrementally\n",
        "    \"Gaussian Naive Bayes\": GaussianNB()\n",
        "}\n",
        "\n",
        "print(\"‚úÖ Models and preprocessors initialized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMG47QgHBMad",
        "outputId": "bc149d6e-b6ff-42c7-95ce-d33351106a89"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Models and preprocessors initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Online Training"
      ],
      "metadata": {
        "id": "SM5vxR2pBa1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. ONLINE TRAINING LOOP ---\n",
        "\n",
        "print(\"üöÄ Starting Online Training...\")\n",
        "chunksize = 200\n",
        "all_classes = np.array([0, 1]) # [0 for normal, 1 for attack]\n",
        "\n",
        "# Read the training data in chunks to simulate a stream\n",
        "for chunk in pd.read_csv(train_file, header=None, names=columns, chunksize=chunksize):\n",
        "    # --- Preprocessing the chunk ---\n",
        "    # 1. Encode categorical features\n",
        "    chunk['protocol_type'] = protocol_encoder.transform(chunk['protocol_type'])\n",
        "    chunk['service'] = service_encoder.transform(chunk['service'])\n",
        "    chunk['flag'] = flag_encoder.transform(chunk['flag'])\n",
        "\n",
        "    # 2. Create the binary target variable (0 for 'normal', 1 for 'attack')\n",
        "    y_chunk = chunk['attack'].apply(lambda x: 0 if x == 'normal' else 1)\n",
        "\n",
        "    # 3. Drop unnecessary columns\n",
        "    X_chunk = chunk.drop(columns=['attack', 'difficulty'])\n",
        "\n",
        "    # 4. Scale numerical features. We use partial_fit to update the scaler's stats.\n",
        "    scaler.partial_fit(X_chunk[numerical_cols])\n",
        "\n",
        "    X_chunk[numerical_cols] = scaler.transform(X_chunk[numerical_cols])\n",
        "    # --- Incremental Training ---\n",
        "    # Update each model with the preprocessed chunk\n",
        "    for model_name, model in models.items():\n",
        "        # The `classes` parameter is required on the first call to `partial_fit`\n",
        "        model.partial_fit(X_chunk, y_chunk, classes=all_classes)\n",
        "\n",
        "print(\"‚úÖ Online Training Complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sx4P37BYBeBc",
        "outputId": "31645dd2-07ab-4f39-c31e-8224885898a4"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting Online Training...\n",
            "‚úÖ Online Training Complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Online Evaluation"
      ],
      "metadata": {
        "id": "KK7VS_GzBldA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. ONLINE EVALUATION ---\n",
        "\n",
        "print(\"\\nüî¨ Starting Online Evaluation on the test set...\")\n",
        "\n",
        "# Dictionary to store the final performance metrics\n",
        "results = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    # Lists to store labels and predictions from all chunks\n",
        "    all_y_true = []\n",
        "    all_y_pred = []\n",
        "\n",
        "    # Process the test data in chunks\n",
        "    for chunk in pd.read_csv(test_file, header=None, names=columns, chunksize=chunksize):\n",
        "        # --- Preprocessing the test chunk ---\n",
        "        # Apply the SAME transformations fitted during training\n",
        "        chunk['protocol_type'] = protocol_encoder.transform(chunk['protocol_type'])\n",
        "        chunk['service'] = service_encoder.transform(chunk['service'])\n",
        "        chunk['flag'] = flag_encoder.transform(chunk['flag'])\n",
        "\n",
        "        y_true = chunk['attack'].apply(lambda x: 0 if x == 'normal' else 1)\n",
        "        X_test_chunk = chunk.drop(columns=['attack', 'difficulty'])\n",
        "\n",
        "        # Use the already fitted scaler to transform the test data\n",
        "        X_test_chunk[numerical_cols] = scaler.transform(X_test_chunk[numerical_cols])\n",
        "\n",
        "        # --- Prediction ---\n",
        "        y_pred = model.predict(X_test_chunk)\n",
        "\n",
        "        # Append chunk results to the main lists\n",
        "        all_y_true.extend(y_true)\n",
        "        all_y_pred.extend(y_pred)\n",
        "\n",
        "    # --- Calculate overall metrics ---\n",
        "    accuracy = accuracy_score(all_y_true, all_y_pred)\n",
        "    precision = precision_score(all_y_true, all_y_pred, average='weighted')\n",
        "    recall = recall_score(all_y_true, all_y_pred, average='weighted')\n",
        "    f1 = f1_score(all_y_true, all_y_pred, average='weighted')\n",
        "\n",
        "    results[model_name] = {\n",
        "        \"Accuracy\": accuracy,\n",
        "        \"Precision\": precision,\n",
        "        \"Recall\": recall,\n",
        "        \"F1-Score\": f1\n",
        "    }\n",
        "    print(f\"  - Evaluation for {model_name} complete.\")\n",
        "\n",
        "print(\"‚úÖ Online Evaluation Complete.\")\n",
        "\n",
        "# --- 6. DISPLAY RESULTS ---\n",
        "print(\"\\n--- üìä Final Model Performance Comparison (Online Learning) üìä ---\")\n",
        "results_df = pd.DataFrame(results).T\n",
        "results_df = results_df.sort_values(by='F1-Score', ascending=False)\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fL4SLQdfBmne",
        "outputId": "7bbd372c-c356-493b-b70e-92a180ee3b8a"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üî¨ Starting Online Evaluation on the test set...\n",
            "  - Evaluation for Online Logistic Regression (SGD) complete.\n",
            "  - Evaluation for Multi-layer Perceptron complete.\n",
            "  - Evaluation for Gaussian Naive Bayes complete.\n",
            "‚úÖ Online Evaluation Complete.\n",
            "\n",
            "--- üìä Final Model Performance Comparison (Online Learning) üìä ---\n",
            "                                  Accuracy  Precision    Recall  F1-Score\n",
            "Multi-layer Perceptron            0.843329   0.871453  0.843329  0.843553\n",
            "Gaussian Naive Bayes              0.770892   0.809196  0.770892  0.770184\n",
            "Online Logistic Regression (SGD)  0.703380   0.814152  0.703380  0.690603\n"
          ]
        }
      ]
    }
  ]
}